[["index.html", "Sổ ghi chép Lời nói đầu", " Sổ ghi chép Lê Huỳnh Đức 2024-03-04 Lời nói đầu "],["feature-engineering.html", "Chương 1 Feature Engineering Giới thiệu Feature Engineering cơ bản Các tài liệu tham khảo Một số thứ xàm xí thu nhặt được", " Chương 1 Feature Engineering Giới thiệu Feature Engineering là quá trình chuyển đổi tập dữ liệu thô ban đầu thành tập dữ liệu có các đặc trưng (features) biểu diễn dữ liệu ban đầu tốt hơn Feature Engineering giúp cải thiện hiệu suất của mô hình cũng như cải thiện độ chính xác của mô hình Các bước thực hiện Feature Engineering bao gồm: Feature Understanding: Là bước hiểu các cột dữ liệu Feature Structuring: Nếu dữ liệu là dữ liệu phi cấu trúc, chúng ta phải chuyển đổi nó về dữ liệu có cấu trúc hoặc dạng dữ liệu mà máy có thể hiểu được Feature Optimization: Sau khi có dữ liệu đã được cấu trúc, bước này giúp tìm kiếm những feature hoặc chọn lọc những feature tối ưu Feature Evaluation: Sau khi đã chọn được các Feature tối ưu, chúng ta đánh giá các lựa chọn bằng cách đánh giá mô hình dự đoán Các loại Feature Engineering: Feature Improvement: Biến đối các feature có sẵn thành các feature dễ hiểu hơn bằng các phép biến đổi toán học Feature Construction: Tạo thêm các feature dễ hiểu từ những feature dễ hiểu khác Feature Selection: Lựa chọn những feature tốt nhất từ tập feature Feature Extraction: Dựa vào thuật toán để tạo những feature mới, đôi khi những feature này không thể giải thích được Feature Learning: Bằng các thuật toán hiện đại, sử dụng các phương pháp pretrained model để tạo tập feature mới từ tập feature đã cho Feature Engineering cơ bản Loại dữ liệu Dữ liệu có cấu trúc Là loại dữ liệu đã được tổ chức thành một cấu trúc và có data model. Dữ liệu thường được biểu diễn dưới dạng bảng với các cột và dòng trong đó mỗi dòng thể hiện một quan sát của dữ liệu và mỗi cột tương ứng với các thuộc tính, đặc trưng của dữ liệu. Ví dụ về các dữ liệu có cấu trúc: - Các bảng trong Cơ sở dữ liệu quan hệ là các dữ liệu có cấu trúc - Các dữ liệu được lưu dưới dạng file CSV, Excel Dữ liệu phi cấu trúc Dữ liệu bán cấu trúc ABC Thông thường, một tập dữ liệu có cả phần có cấu trúc và phi cấu trúc. Ví dụ: Nếu chúng ta đang xử lý tập dữ liệu về các cuộc gọi điện thoại. Tập hợp con của dữ liệu này bao gồm ngà thực hiện của gọi, tên người thực cuộc gọi, địa điệm thực hiện cuộc gọi, người nhận cuộc gọi là dữ liệu có cấu trúc. Tập ghi âm thô của cuộc gọi sẽ là dữ liệu phi cấu trúc Ví dụ Phần cấu trúc Phần phi cấu trúc Cuộc gọi thoại Người gọi, người nhận, thời gian gọi, địa điểm gọi, mục đích gọi Bản ghi âm cuộc gọi Form bảo hiểm Ngày tháng, người điền thông tin, mục muốn bồi thường Nội dung các câu hỏi Job Description Tên công ty, vị trí cần tuyển, số năm kinh nghiệm yêu cầu Nội dung từng phần Các tài liệu tham khảo Feature Engineering and Selection: A Practical Approach for Predictive Models Một số thứ xàm xí thu nhặt được “Dự đoán xấp xỉ” - Approximate Predictions giúp Feature Selection nhanh hơn link Nhắc lại về SHAP Value, SHAP value thể hiện gía trị đóng góp của mỗi feature cho từng quan sát Thay vì chọn candidated features sau đó train lại, thì chọn candidated features sau đó dùng bảng SHAP để ước lượng giá trị dự đoán. Như thế sẽ giảm được thời gian đào tạo. Bài viết đã so sánh 2 cách: (A) : Train lại trên tập candidated features, predict trên tập valid (B) : Dựa trên bảng SHAP, tính toán luôn Sau đó so sánh correlation của 2 phương án A, B trên các tập dataset khác nhau. Kết quả giá trị tối thiểu của Rho là 0.87 và giá trị tối đa là 1 "],["time-series-analysis.html", "Chương 2 Time Series Analysis", " Chương 2 Time Series Analysis "],["anormaly-detection.html", "Chương 3 Anormaly Detection", " Chương 3 Anormaly Detection "],["data-storytelling-cho-dự-án-data-science.html", "Chương 4 Data Storytelling cho dự án Data Science 4.1 Sự khác biệt giữa Data Visualization và Data Storytelling 4.2 Tại sao lại cần Data Storytelling? Data Visualization vẫn chưa đủ? 4.3 Điểm mạnh của Data Storytelling so với Data Visualization", " Chương 4 Data Storytelling cho dự án Data Science 4.1 Sự khác biệt giữa Data Visualization và Data Storytelling Ngữ cảnh DataViz Ngữ cảnh Data Storytelling I picked 8 apples and made 2 pies to sell. I picked 8 apples from my grandfather’s garden, who grew them naturally. The apples I harvested were transported by my grandfather and I in his hybrid car. We took them to a world-class kitchen where I invited the best cook in the country to make 2 apple pies from scratch, which were reserved for purchase by our town’s mayor to give to his mother on Mother’s Day. Want to know what he thought of them? 4.2 Tại sao lại cần Data Storytelling? Data Visualization vẫn chưa đủ? 4.3 Điểm mạnh của Data Storytelling so với Data Visualization Khi chúng ta kể chuyện, chúng ta đưa cảm xúc vào dữ liệu và liên kết nó lại với nhau. "],["các-vấn-đề-liên-quan-đến-dữ-liệu.html", "Chương 5 Các vấn đề liên quan đến dữ liệu 5.1 Xử lý dữ liệu mất cân bằng", " Chương 5 Các vấn đề liên quan đến dữ liệu 5.1 Xử lý dữ liệu mất cân bằng 5.1.1 Cách 1: Sử dụng Class weights LGBMClassifier(class_weight=&#39;balanced&#39;) XGBClassifier(scale_pos_weight=4.71) CatBoostClassifier(auto_class_weights=&#39;Balanced&#39;) LogisticRegression(class_weight=&#39;balanced&#39;) LinearDiscriminantAnalysis(priors=[0.5, 0.5]) 5.1.2 Cách 2: Sử dụng phương pháp Downsample Lớp Negative Cách thứ 2 để cân bằng các lớp là sử dụng toàn bộ dữ liệu của tập positive và chỉ sử dụng một phần của dữ liệu negative. Cách này được gọi là Downsampling. Vì chúng ta không sử dụng toàn bộ dữ liệu của lớp negative trong việc huấn luyện model, do đó chúng ta sẽ huấn luyện nhiều model với mỗi lần downsample khác nhau. Dưới đây là code ví dụ from imblearn.under_sampling import RandomUnderSampler NUM_POS = df.Class.sum() sampler = RandomUnderSampler(sampling_strategy={0: NUM_POS, 1: NUM_POS}) X_train, y_train = sampler.fit_resample(df[FEATURES], df[&#39;Class&#39;]) Hoặc có thể làm như sau cho cross validation 5.1.3 Cách 3: Xử lý kết quả dự đoán Nếu bạn không muốn thay đổi trọng số cho lớp positive trong lúc huấn luyện, bạn có thể xử lý kết quả dự đoán boost = 4.7 # Thay đổi phù hợp với dữ liệu odds = boost * preds / (1-preds) preds = odds / (1+odds) trong đó preds là xác suất dự đoán giá trị postitive=1 5.1.4 Cách 4: Sử dụng hàm loss cân bằng ví dụ \\[\\begin{align} \\Large LogLoss = \\frac{-\\frac{1}{N_0}\\sum_{i=1}^{n_0} y_{0i}logp_{0i} -\\frac{1}{N_1}\\sum_{i=1}^{n_1} y_{1i}logp_{1i}}{2} \\end{align}\\] "],["deployment.html", "Chương 6 Deployment 6.1 Tích hợp ChatGPT vào Gapo", " Chương 6 Deployment 6.1 Tích hợp ChatGPT vào Gapo "],["xử-lý-data-drift-và-concept-drift.html", "Chương 7 Xử lý Data Drift và Concept Drift 7.1 Data drift là gì", " Chương 7 Xử lý Data Drift và Concept Drift 7.1 Data drift là gì "],["mlops.html", "Chương 8 MLOPS 8.1 MLOPS là gì 8.2 Các frameworks triển khai MLOPS 8.3 Giới thiệu MLlfow 8.4 Giới thiệu Metaflow", " Chương 8 MLOPS 8.1 MLOPS là gì 8.2 Các frameworks triển khai MLOPS 8.3 Giới thiệu MLlfow 8.4 Giới thiệu Metaflow "],["phân-tích-đồ-thị.html", "Chương 9 Phân tích đồ thị 9.1 Đồ thị là gì 9.2 Thư viện folium 9.3 Bài toán phân tích location", " Chương 9 Phân tích đồ thị 9.1 Đồ thị là gì 9.2 Thư viện folium import folium vietnam_map = folium.Map(location=[14.0583, 108.2772], zoom_start=6) for _ , row in locations.iterrows(): folium.CircleMarker( location=[row[&quot;LAT_&quot;], row[&quot;LONG_&quot;]], radius=10, color=row[&#39;Color&#39;], fill=True, fill_opacity=0.6 ).add_to(vietnam_map) folium.map.Marker( [row[&quot;LAT_&quot;], row[&quot;LONG_&quot;]], icon=folium.DivIcon( icon_size=(60, 20), html=f&quot;&lt;div style=&#39;font-size: 50px; color:red&#39;&gt;{row[&#39;DIEM_MAT_BANG&#39;]}&lt;/div&gt;&quot;, ), popup=f&quot;&quot;&quot;&lt;div style=&#39;width: 200px;&#39;&gt;SHOP_WID: {int(row[&quot;SHOP_WID&quot;])} &lt;br&gt;Số lượng hợp đồng giải ngân: {int(row[&quot;SLHD_GN&quot;])} &lt;br&gt;Ngày thống kê: {row[&#39;MONTH_&#39;]} &lt;br&gt;Close_date:{row[&#39;CLOSE_DT&#39;]} &lt;/div&gt; &quot;&quot;&quot;, ).add_to(vietnam_map) marker1_coords = (21.02684, 105.833870) marker2_coords = (21.034494, 105.831664) marker3_coords = (21.018974, 105.838349) marker4_coords = (21.021274, 105.825743) folium.PolyLine([marker1_coords, marker2_coords], color=&quot;green&quot;, popup=f&quot;Khoảng cách: {int(geodesic(marker1_coords, marker2_coords).meters)}&quot;).add_to(vietnam_map) folium.PolyLine([marker1_coords, marker3_coords], color=&quot;green&quot;, popup=f&quot;Khoảng cách: {int(geodesic(marker1_coords, marker3_coords).meters)}&quot;).add_to(vietnam_map) folium.PolyLine([marker1_coords, marker4_coords], color=&quot;green&quot;, popup=f&quot;Khoảng cách: {int(geodesic(marker1_coords, marker4_coords).meters)}&quot;).add_to(vietnam_map) # Display the map # vietnam_map vietnam_map.save(&quot;../outputs/map.html&quot;) 9.3 Bài toán phân tích location 9.3.1 Thuật toán Point-in-Polygon Thuật toán Point-in-Polygon cho phép đếm số lượng các point nằm trong một đa giác, từ đó so sánh mật độ ### Phân tích tiệm cận Proximi analysis sử dụng thuật toán ball-tree "],["cac-loai-model.html", "Chương 10 Các loại model 10.1 Causal Inference 10.2 Survival Model 10.3 DEA Base Model", " Chương 10 Các loại model 10.1 Causal Inference Ba cấp độ Association Intervention Counterfactuals Bốn keysteps của causal inference Modeling: Create a causal graph with important features Identification: Establish an approach to estimate the effect and identify with features to control for Estimation: Estimate the effect using statistical techniques Refutation: Validate the assumption and estimates https://medium.com/@ryutayoshimatsu/causal-machine-learning-a3dc79205674 Topics: Adjustment using Back-door and Front-door critiera Selection bias Instrumental variable Potential outcomes and couterfactual Causal discovery Causal Reinforcement Learning 10.2 Survival Model 10.3 DEA Base Model https://medium.com/analytics-vidhya/introduction-to-data-envelopment-analysis-in-r-773745549d6a 10.3.1 Các thuật ngữ cần nắm Input-Oriented Model Output-Oriented Model DEA Frontier How to evaluate the efficiency? 10.3.2 Mở đầu Năm 1978, Charnes, Cooper and Rhodes đã đề xuất mô hình DEA, CCR model đầu tiên. Họ đã mở rộng mô hình từ một đầu vào, một đầu ra thành nhiều đầu vào và nhiều đầu ra. Mô hình CCR bây giờ đã trở thành một mô hình quen thuộc để đánh giá hiệu suất. Công thức của mô hình như sau Maximize \\[ \\Large DMU(k)= max \\frac{\\sum_{r=1}^{q}{u_r y_{rk}}}{\\sum^{m}_{i=1}{v_i x_{ik}}} \\] Subject to \\[ \\Large \\frac{\\sum^{q}_{r=1}{u_r y_{rj}}}{\\sum^{m}_{i=1}{v_i x_{ij}}} \\le 1 , \\forall j \\] \\[ v&gt;0;u \\ge 0;i=1,2,⋯,m;r=1,2,⋯,q \\] Với mỗi quyết định cho một đơn vị \\(k\\) trong tập đơn vị \\(S\\) sẽ có \\(m\\) input được định nghĩa là \\(x_i(i=1, 2, 3, ..., m)\\) với mỗi \\(x_i\\) sẽ có trọng số là \\(v_i\\) \\(q\\) output được định nghĩa là \\(y_r(i=1, 2, 3, ..., q)\\) với mỗi \\(y_r\\) sẽ có trọng số là \\(u_r\\) \\(j\\) là đơn vị trong tập \\(S\\) Vì phương trình này là phi tuyến tính và có vô số nghiệm tối ưu. Trong thực tế, cần phải chuyển đổi đó thành Linear Programing Primary Form Objective \\[ \\Large DMU(k)= max {\\sum_{r=1}^{q}{u_r y_{rk}}} \\] Subject to \\[ \\Large {\\sum^{q}_{r=1}{u_r y_{rj}}} - {\\sum^{m}_{i=1}{v_i x_{ij}}} \\le 0 , \\forall j \\] \\[ \\Large {\\sum^{m}_{i=1}{v_i x_{ik}}} = 1 \\] \\[ v&gt;0; \\ge 0; i=1,2,⋯,m; r=1,2,⋯,q \\] Dual Model Objective \\[ \\Large min \\ \\theta \\] Subject to \\[ \\Large \\sum_{j=1}^{n}{\\lambda_j x_{ij}} \\le \\theta x_{ik} \\\\ \\Large \\sum_{j=1}^{n}{\\lambda_j y_{rj} \\ge y_{rk}} \\\\ \\Large 0 &lt; \\theta \\le 1; \\lambda \\ge 0; i =1,2,3,...,m; r=1,2,...,q; j=1,2,...,n; k=1,2,...s \\] Mô hình CCR giả định lợi nhuận không đổi theo quy mô, tức là đầu vào của đơn vị quyết định tăng lên t lần kích thước ban đầu của nó và đầu ra của nó cũng gấp t lần kích thước ban đầu. Tuy nhiên, giả định rằng tất cả các đơn vị quyết định đều ở quy mô sản xuất tối ưu không phù hợp với tình hình thực tế của hầu hết các đơn vị quyết định. Trên cơ sở đó, Banker, Charnes và Cooper [39] lần đầu tiên đề xuất mô hình BCC (được đặt theo tên viết tắt của ba tác giả) để xác định xem đơn vị quyết định có đạt được quy mô sản xuất hiệu quả hay không, đo lường cả hiệu quả quy mô và hiệu quả kỹ thuật. BCC đề xuất thêm một constraint \\(\\sum^{n}_{j}\\lambda_{j}=(\\lambda &gt;=0)\\) \\[ \\Large min \\ \\theta \\] Subject to \\[ \\Large \\sum_{j=1}^{n}{\\lambda_j x_{ij}} \\le \\theta x_{ik} \\\\ \\Large \\sum_{j=1}^{n}{\\lambda_j y_{rj} \\ge y_{rk}} \\\\ \\sum^{n}_{j}\\lambda_j=1 \\\\ \\Large 0 &lt; \\theta \\le 1; \\lambda \\ge 0; i =1,2,3,...,m; r=1,2,...,q; j=1,2,...,n; k=1,2,...s \\] "],["tips.html", "Chương 11 TIPS 11.1 Train và Test", " Chương 11 TIPS Một số cạm bẫy 11.1 Train và Test "],["các-đầu-sách-và-paper.html", "Chương 12 Các đầu sách và Paper 12.1 Các đầu sách 12.2 Các link bài báo", " Chương 12 Các đầu sách và Paper 12.1 Các đầu sách 12.1.1 Feature Engineering and Selection: A Practical Approach for Predictive Models link 12.1.2 Flexible Imputation of Missing Data link 12.1.3 Feature Engineering and Selection: A Practical Approach for Predictive Models link 12.1.4 Causal Inference: What If link 12.1.5 Dataset Shift in Machine Learning link 12.1.6 Interpretable Machine Learning link 12.1.7 Causal Inference and Discovery in Python – Machine Learning and Pearlian Perspective link 12.2 Các link bài báo 12.2.1 Time-Aware Transformer-based Network for Clinical Notes [paper] - [code] 12.2.2 Long-term Forecasting with TiDE: Time-series Dense Encoder [paper] - [code] 12.2.3 Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning [paper] 12.2.4 Cyclic Boosting - an explainable supervised machine learning algorithm [paper] - [code] - [code] 12.2.5 TSMixer: An All-MLP Architecture for Time Series Forecasting [paper] - [code] - [example] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
