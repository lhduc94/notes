# Các vấn đề liên quan đến dữ liệu
## Xử lý dữ liệu mất cân bằng {-}

### Cách 1: Sử dụng Class weights {-}

```python
LGBMClassifier(class_weight='balanced')
XGBClassifier(scale_pos_weight=4.71)
CatBoostClassifier(auto_class_weights='Balanced')
LogisticRegression(class_weight='balanced')
LinearDiscriminantAnalysis(priors=[0.5, 0.5])
```

### Cách 2: Sử dụng phương pháp Downsample Lớp Negative {-}

Cách thứ 2 để cân bằng các lớp là sử dụng toàn bộ dữ liệu của tập `positive` và chỉ sử dụng một phần của dữ liệu `negative`. Cách này được gọi là `Downsampling`. Vì chúng ta không sử dụng toàn bộ dữ liệu của lớp `negative` trong việc huấn luyện model, do đó chúng ta sẽ huấn luyện nhiều model với mỗi lần downsample khác nhau.

Dưới đây là code ví dụ
```python
from imblearn.under_sampling import RandomUnderSampler
NUM_POS = df.Class.sum()
sampler = RandomUnderSampler(sampling_strategy={0: NUM_POS, 1: NUM_POS})
X_train, y_train = sampler.fit_resample(df[FEATURES], df['Class'])
```

Hoặc có thể làm như sau cho cross validation
![image](../images/05/05x01.png)


### Cách 3: Xử lý kết quả dự đoán {-}
Nếu bạn không muốn thay đổi trọng số cho lớp `positive` trong lúc huấn luyện, bạn có thể xử lý kết quả dự đoán

```python
boost = 4.7 # Thay đổi phù hợp với dữ liệu
odds = boost * preds / (1-preds)
preds = odds / (1+odds)
```
trong đó preds là xác suất dự đoán giá trị `postitive=1`

### Cách 4: Sử dụng hàm loss cân bằng {-}
ví dụ

$$\begin{align}
 \Large LogLoss = \frac{-\frac{1}{N_0}\sum_{i=1}^{n_0} y_{0i}logp_{0i} -\frac{1}{N_1}\sum_{i=1}^{n_1} y_{1i}logp_{1i}}{2}
\end{align}$$